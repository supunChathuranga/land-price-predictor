# -*- coding: utf-8 -*-
"""ML_Group_G_Assingment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rdDRunZvN_w6Lrq1o_t80w4UlSMY1JMJ

# **Step 1: Loading the Dataset**

**Importing Required Libraries:**  
In this step, we import the necessary libraries, such as pandas, to handle data manipulation and analysis throughout the assignment.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""**Loading the Dataset:**
Then we use pandas to load the dataset into Python. The information is kept in a single-sheet Excel file. To comprehend the structure and characteristics of the data, we will examine it.
"""

# Load the dataset
file_path = 'Land_prices_of_Colombo_district-Sri_Lanka.xlsx'
xls = pd.ExcelFile(file_path)

# Load the first sheet into a DataFrame
df = pd.read_excel(file_path, sheet_name='Sheet1')

# Display the first few rows to inspect the data
df.head()

"""#**Step 2: Inspect the Dataset**

**Exploring the Dataset:**
We'll look at the data types, structure, and existence of missing or unnecessary information in the dataset. Planning the preprocessing stage will be easier if we are aware of these factors.
"""

# Get basic information about the dataset
df.info()

# Check for missing values
df.isnull().sum()

# Summary statistics for numerical columns
df.describe()

"""# Step 3: Data Cleaning

**Data Cleaning:**
We'll deal with missing values, eliminate unnecessary columns, and ensure that all data types are accurate. We'll concentrate on the "Distance from Fort" and "Price per Perch" columns for this project.
"""

# Strip any leading/trailing whitespace from column names
df.columns = df.columns.str.strip()

# Drop unwanted columns
df = df.drop(columns=['Address_ID', 'Address', 'Land_type', 'Posted_Date_new', 'Mentioned Price(Rs)', 'min_dist_govtschools_b', 'min_dist_nearest_Pvt_Med_center', 'min_dist_nearest_FinanceCompany' ])



# Reset the index after dropping rows
df.reset_index(drop=True, inplace=True)

# Display the cleaned dataset
df.head()

"""#Step 4: Check for Outliers

**Checking for Outliers:**
Outliers can skew the results of our model. We'll visualize the data to identify any outliers in the "Price per Perch" and "Distance from Fort" columns.
"""

df.info()

df.columns

"""**Visualizing Data Distribution:**  
This step plots a histogram and KDE curve to analyze the distribution of a specific feature, helping to identify patterns or outliers in the data.

"""

plt.figure(figsize=(10, 6))
sns.histplot(df['Distance from fort'], kde=True, bins=20, color='blue', alpha=0.7)
plt.title('Distribution of Distance from Fort')
plt.xlabel('Distance from Fort (km)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""# **Step 5: Visualizing Data Distribution**

**Visualizing Data Distribution:**  
This step plots a histogram and KDE curve to analyze the distribution of a specific feature, helping to identify patterns or outliers in the data.

"""

plt.figure(figsize=(10, 6))
sns.histplot(df['min_dist_govtschools_a'], kde=True, bins=20, color='red', alpha=0.7)
plt.title('min_dist_govt/schools_a')
plt.xlabel('min_dist_govtschools_a(km)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""# **Step 6: Visualizing Relationships Between Features**

**Scatter Plot:**  
In this stepp, we create a scatter plot to visualize the relationship between the distance from Fort (`Distance from fort`) and the price per perch (`Price per Perch`). This helps us to:
1. Identify any patterns or trends in how land prices vary with distance from Fort.
2. Detect outliers or anomalies in the data that might affect model performance.
3. Gain insights into potential correlations between these features, which can guide feature selection for modeling.  

The plot includes adjustments for axis limits, gridlines, and transparency to ensure clarity and readability.


"""

# Scatter plot to visualize the relationship between 'Distance from Fort' and 'Price per Perch'
plt.figure(figsize=(10, 6))
plt.scatter(df['Distance from fort'], df['Price per Perch'], alpha=0.7)

# Set the title and labels
plt.title('Scatter Plot: Distance from Fort vs. Price per Perch')
plt.xlabel('Distance from Fort (km)')
plt.ylabel('Price per Perch (Rs)')

plt.xlim(0, 40)
plt.ylim(0, 45000000)

# Enable the grid
plt.grid(True)

# Show the plot
plt.show()

"""# **Step 7: Analyzing Correlations**

**Correlation Heatmap:**  
In this step, we generate a heatmap to analyze the correlation between `Distance from fort` and `Price per Perch`. This visualization helps us:
1. Quantify the strength and direction of the relationship between these two features.
2. Understand if `Distance from fort` has a positive or negative impact on land pricing.
3. Identify whether this feature is relevant for predicting `Price per Perch` in our machine learning model.  

The heatmap uses a color gradient to represent correlation values, with annotations for precise interpretation.

"""

# Select the columns of interest
correlation_columns = ['Distance from fort', 'Price per Perch']

# Compute the correlation matrix
corr_matrix = df[correlation_columns].corr()

# Plot the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='Oranges', fmt='.2f', cbar=True)

# Set the title
plt.title('Correlation Heatmap: Distance from Fort (km) vs. Price per Perch')

# Show the plot
plt.show()

"""# **Step 8: Visualizing the Effect of Proximity to Government Schools on Price**

**Scatter Plot:**  
To look into the link between the price per perch (`Price per Perch`) and the minimum distance to government schools (`min_dist_govtschools_a`), we create a scatter plot in this step. This visualization helps us:
1. Examine if proximity to government schools has a significant impact on land pricing.
2. Identify patterns, trends, or outliers that might influence the pricing model.
3. Gain insights into the correlation between these two variables for feature selection.  

The plot includes adjustments for axis limits, gridlines, and transparency to ensure readability and clarity.

"""

# Scatter plot to visualize the relationship between 'min_dist_govtschools_a' and 'Price per Perch'
plt.figure(figsize=(10, 6))
plt.scatter(df['min_dist_govtschools_a'], df['Price per Perch'], alpha=0.7)

# Set the title and labels
plt.title('Scatter Plot: min_dist_govtschools_a vs. Price per Perch')
plt.xlabel('min_dist_govtschools_a(km)')
plt.ylabel('Price per Perch (Rs)')

plt.xlim(0, 30)
plt.ylim(0, 45000000)

# Enable the grid
plt.grid(True)

# Show the plot
plt.show()

"""# **Step 9: Exploring the Impact of Semi-Government School Proximity on Price**

**Scatter Plot:**  
In this step, now we visualize the relationship between the minimum distance to semi-government schools (`min_dist_semigovtschools`) and the price per perch (`Price per Perch`). This scatter plot helps us:
1. Investigate whether being closer to semi-government schools affects land prices significantly.
2. Detect patterns or outliers that might influence the data's overall trend.
3. Evaluate the relevance of this feature for predicting land prices in our machine learning model.  

Same as before this plot includes customized axis limits, gridlines, and transparency for better clarity and interpretation.

"""

# Scatter plot to visualize the relationship between 'min_dist_semigovtschools_b' and 'Price per Perch'
plt.figure(figsize=(10, 6))
plt.scatter(df['min_dist_semigovtschools'], df['Price per Perch'], alpha=0.7)

# Set the title and labels
plt.title('Scatter Plot: min_dist_semigovtschools vs. Price per Perch')
plt.xlabel('min_dist_semigovtschools(km)')
plt.ylabel('Price per Perch (Rs)')

plt.xlim(0, 3)
plt.ylim(0, 60000000)


# Enable the grid
plt.grid(True)

# Show the plot
plt.show()

"""# **Step 10: Analyzing the Proximity to Expressways and Land Prices**

**Scatter Plot:**  
The relationship between the price per perch (`Price per Perch`) and the minimum distance to the closest expressway (`min_dist_nearest_express`) is studied in this stage by creating another scatter plot. This visualization helps us:
1. Understand if distance expressways influences land pricing significantly.
2. Identify any trends or patterns that suggest expressway distance as a key factor in land valuation.
3. Detect outliers or anomalies in the dataset that could impact model performance.  

"""

# Scatter plot to visualize the relationship between 'min_dist_nearest_express' and 'Price per Perch'
plt.figure(figsize=(10, 6))
plt.scatter(df['min_dist_nearest_express'], df['Price per Perch'], alpha=0.7)

# Set the title and labels
plt.title('Scatter Plot: min_dist_nearest_express vs. Price per Perch')
plt.xlabel('min_dist_nearest_express(km)')
plt.ylabel('Price per Perch (Rs)')

plt.xlim(0, 3)
plt.ylim(0, 60000000)


# Enable the grid
plt.grid(True)

# Show the plot
plt.show()

"""
markdown
Copy code
# **Step 11: Checking Railway Proximity and Land Prices**

**Scatter Plot:**  
This step shows the relationship between the distance to the nearest railway (`min_dist_nearest_railway`) and the price per perch (`Price per Perch`). It helps us:
1. See if being closer to railways affects land prices.
2. Spot any patterns or unusual data points.
3. Decide if this feature is useful for predicting land prices."""

# Scatter plot to visualize the relationship between 'min_dist_nearest_railway' and 'Price per Perch'
plt.figure(figsize=(10, 6))
plt.scatter(df['min_dist_nearest_railway'], df['Price per Perch'], alpha=0.7)

# Set the title and labels
plt.title('Scatter Plot: min_dist_nearest_railway vs. Price per Perch')
plt.xlabel('min_dist_nearest_railway(km)')
plt.ylabel('Price per Perch (Rs)')

plt.xlim(0, 3)
plt.ylim(0, 60000000)


# Enable the grid
plt.grid(True)

# Show the plot
plt.show()

"""# **Step 12: Checking Bank Proximity and Land Prices**

**Scatter Plot:**  
This step shows how the distance to the nearest bank (`min_dist_nearest_bank`) relates to the price per perch (`Price per Perch`). It helps us:
1. Understand if being closer to banks impacts land prices.
2. Identify any patterns or unusual points in the data.
3. Decide if this feature is useful for predicting land prices.  

The plot is adjusted with clear limits and gridlines for easy interpretation.

"""

# Scatter plot to visualize the relationship between 'min_dist_nearest_bank' and 'Price per Perch'
plt.figure(figsize=(10, 6))
plt.scatter(df['min_dist_nearest_bank'], df['Price per Perch'], alpha=0.7)

# Set the title and labels
plt.title('Scatter Plot: min_dist_nearest_bank vs. Price per Perch')
plt.xlabel('min_dist_nearest_bank(km)')
plt.ylabel('Price per Perch (Rs)')

plt.xlim(0, 3)
plt.ylim(0, 60000000)


# Enable the grid
plt.grid(True)

# Show the plot
plt.show()

"""# **Step 13: Checking Government Hospital Proximity and Land Prices**

**Scatter Plot:**  
This step displays the correlation between the price per perch (`Price per Perch`) and the distance to the closest government hospital (`min_dist_nearest_Govt_Hospital`). It benefits us:
1. Examine whether land values are impacted by proximity to government hospitals.
2. Examine the data for any oddities or patterns.
3. Determine the significance of this attribute for land price prediction.  

Limits and gridlines are added to the plot to improve readability.

"""

# Scatter plot to visualize the relationship between 'min_dist_nearest_Govt_Hospital' and 'Price per Perch'
plt.figure(figsize=(10, 6))
plt.scatter(df['min_dist_nearest_Govt_Hospital'], df['Price per Perch'], alpha=0.7)

# Set the title and labels
plt.title('Scatter Plot: min_dist_nearest_Govt_Hospital vs. Price per Perch')
plt.xlabel('min_dist_nearest_Govt_Hospital(km)')
plt.ylabel('Price per Perch (Rs)')

plt.xlim(0, 3)
plt.ylim(0, 60000000)


# Enable the grid
plt.grid(True)

# Show the plot
plt.show()

"""# **Step 14: Checking Private Hospital Proximity and Land Prices**

**Scatter Plot:**  
This phase looks at how the price per perch (`Price per Perch`) and the distance to the closest private hospital (`min_dist_nearest_Pvt_Hospital`) relate to each other. It benefits us:

1. Find out if land values are affected by proximity to private hospitals.
2. Look for patterns or anomalies in the data.
3. Determine the importance of this attribute for land price prediction.  


"""

# Scatter plot to visualize the relationship between 'min_dist_nearest_Pvt_Hospital' and 'Price per Perch'
plt.figure(figsize=(10, 6))
plt.scatter(df['min_dist_nearest_Pvt_Hospital'], df['Price per Perch'], alpha=0.7)

# Set the title and labels
plt.title('Scatter Plot: min_dist_nearest_Pvt_Hospital vs. Price per Perch')
plt.xlabel('min_dist_nearest_Pvt_Hospital(km)')
plt.ylabel('Price per Perch (Rs)')

plt.xlim(0, 3)
plt.ylim(0, 60000000)


# Enable the grid
plt.grid(True)

# Show the plot
plt.show()

"""# **Step 15: Checking Supermarket Proximity and Land Prices**

**Scatter Plot:**  
This stage investigates the connection between the price per perch (`Price per Perch`) and the distance to the closest supermarket (`min_dist_nearest_Supermarket`). It benefits us:

1. Examine whether land costs are impacted by a store's nearby.
2. Look for trends or anomalies in the data.
3. Determine whether this attribute is important for land price prediction.  

For improved visualization, the plot is modified using gridlines and axis restrictions.

"""

# Scatter plot to visualize the relationship between 'min_dist_nearest_Supermarket' and 'Price per Perch'
plt.figure(figsize=(10, 6))
plt.scatter(df['min_dist_nearest_Supermarket'], df['Price per Perch'], alpha=0.7)

# Set the title and labels
plt.title('Scatter Plot: min_dist_nearest_Supermarket vs. Price per Perch')
plt.xlabel('min_dist_nearest_Supermarket(km)')
plt.ylabel('Price per Perch (Rs)')

plt.xlim(0, 2)
plt.ylim(0, 50000000)


# Enable the grid
plt.grid(True)

# Show the plot
plt.show()

"""# **Step 16: Checking Fuel Station Proximity and Land Prices**

**Scatter Plot:**  
This step investigates the relationship between the distance to the nearest fuel station (`min_dist_nearest_Fuel_station`) and the price per perch (`Price per Perch`). It helps us:
1. Determine if being closer to fuel stations affects land prices.
2. Identify patterns or unusual data points in the dataset.
3. Decide if this feature should be included in the land price prediction model.

"""

# Scatter plot to visualize the relationship between 'min_dist_nearest_Fuel_station' and 'Price per Perch'
plt.figure(figsize=(10, 6))
plt.scatter(df['min_dist_nearest_Fuel_station'], df['Price per Perch'], alpha=0.7)

# Set the title and labels
plt.title('Scatter Plot: min_dist_nearest_Fuel_station vs. Price per Perch')
plt.xlabel('min_dist_nearest_Fuel_station(km)')
plt.ylabel('Price per Perch (Rs)')

plt.xlim(0, 3)
plt.ylim(0, 60000000)


# Enable the grid
plt.grid(True)

# Show the plot
plt.show()

"""# **Step 17: Analyzing Correlation Between Fuel Station Proximity and Land Prices**

**Correlation Heatmap:**  
The correlation between the price per perch (`Price per Perch`) and the distance to the closest fuel station (`min_dist_nearest_Fuel_station`) is calculated and displayed in this step. It benefits us:

1. Recognize how strongly and in which direction these two attributes are related.
2. Find out if land prices are significantly impacted by a property's closeness to filling stations.
3. Determine if the prediction model should make use of this feature.  

With the use of annotations and a color gradient, the heatmap clearly displays the correlation values.

"""

# Select the columns of interest
correlation_columns = ['min_dist_nearest_Fuel_station', 'Price per Perch']

# Compute the correlation matrix
corr_matrix = df[correlation_columns].corr()

# Plot the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', cbar=True)

# Set the title
plt.title('Correlation Heatmap: min_dist_nearest_Fuel_station vs. Price per Perch')

# Show the plot
plt.show()

"""# **Step 18: Detecting Outliers with Boxplots**

**Boxplots:**  
In this stage, we visualize the distribution and identify outliers for different dataset features using boxplots. This helps us:

1. Determine which extreme values might affect the results of the research or the model's predictions.
2. Recognize each feature's central tendency and distribution.
3. Choose possible outlier handling techniques, including transformation or removal.

The boxplots are generated for the following features:
- `Price per Perch`: To check for unusual pricing values.
- `Distance from Fort`: To identify outlier distances from the central point.
- `Land_size(Perches)`: To analyze unusual land sizes.
- Proximity metrics, such as distances to government schools, semi-government schools, universities, expressways, railways, banks, hospitals (both government and private), supermarkets, and fuel stations.

Each boxplot is customized with labels and dimensions to ensure clear visualization of the feature-specific data distributions.

"""

#to see the outliers
plt.figure(figsize=(10, 5))
plt.boxplot(df['Price per Perch'], vert=False)
plt.title('Boxplot of Price per Perch')
plt.xlabel('Price per Perch')
plt.show()

# Boxplot for "Distance from Fort"
plt.figure(figsize=(10, 5))
plt.boxplot(df['Distance from fort'], vert=False)
plt.title('Boxplot of Distance from Fort')
plt.xlabel('Distance from Fort')
plt.show()

# Boxplot for "Land_size(Perches) "
plt.figure(figsize=(10, 5))
plt.boxplot(df['Land_size(Perches)'], vert=False)
plt.title('Boxplot of Land_size(Perches) ')
plt.xlabel('Land_size(Perches) ')
plt.show()

# Boxplot for "min_dist_govtschools_a"
plt.figure(figsize=(10, 5))
plt.boxplot(df['min_dist_govtschools_a'], vert=False)
plt.title('Boxplot of min_dist_govtschools_a ')
plt.xlabel('min_dist_govtschools_a ')
plt.show()



# Boxplot for "min_dist_semigovtschools "
plt.figure(figsize=(10, 5))
plt.boxplot(df['min_dist_semigovtschools'], vert=False)
plt.title('min_dist_semigovtschools ')
plt.xlabel('min_dist_semigovtschools ')
plt.show()

# Boxplot for "min_dist_intlschools "
plt.figure(figsize=(10, 5))
plt.boxplot(df['min_dist_intlschools'], vert=False)
plt.title('Boxplot of min_dist_intlschools ')
plt.xlabel('min_dist_intlschools ')
plt.show()

# Boxplot for "min_dist_uni "
plt.figure(figsize=(10, 5))
plt.boxplot(df['min_dist_uni'], vert=False)
plt.title('Boxplot of min_dist_uni ')
plt.xlabel('min_dist_uni ')
plt.show()

# Boxplot for "min_dist_nearest_express "
plt.figure(figsize=(10, 5))
plt.boxplot(df['min_dist_nearest_express'], vert=False)
plt.title('Boxplot of min_dist_nearest_express ')
plt.xlabel('min_dist_nearest_express ')
plt.show()

# Boxplot for "min_dist_nearest_railway "
plt.figure(figsize=(10, 5))
plt.boxplot(df['min_dist_nearest_railway'], vert=False)
plt.title('Boxplot of min_dist_nearest_railway ')
plt.xlabel('min_dist_nearest_railway ')
plt.show()

# Boxplot for "min_dist_nearest_bank "
plt.figure(figsize=(10, 5))
plt.boxplot(df['min_dist_nearest_bank'], vert=False)
plt.title('min_dist_nearest_bank ')
plt.xlabel('min_dist_nearest_bank ')
plt.show()



# Boxplot for "min_dist_nearest_Govt_Hospital "
plt.figure(figsize=(10, 5))
plt.boxplot(df['min_dist_nearest_Govt_Hospital'], vert=False)
plt.title('Boxplot of min_dist_nearest_Govt_Hospital ')
plt.xlabel('min_dist_nearest_Govt_Hospital ')
plt.show()

# Boxplot for "min_dist_nearest_Pvt_Hospital "
plt.figure(figsize=(10, 5))
plt.boxplot(df['min_dist_nearest_Pvt_Hospital'], vert=False)
plt.title('Boxplot of min_dist_nearest_Pvt_Hospital ')
plt.xlabel('min_dist_nearest_Pvt_Hospital ')
plt.show()


# Boxplot for "min_dist_nearest_Supermarket "
plt.figure(figsize=(10, 5))
plt.boxplot(df['min_dist_nearest_Supermarket'], vert=False)
plt.title('Boxplot of min_dist_nearest_Supermarket ')
plt.xlabel('min_dist_nearest_Supermarket ')
plt.show()

# Boxplot for "min_dist_nearest_Fuel_station"
plt.figure(figsize=(10, 5))
plt.boxplot(df['min_dist_nearest_Fuel_station'], vert=False)
plt.title('Boxplot of min_dist_nearest_Fuel_station ')
plt.xlabel('min_dist_nearest_Fuel_station ')
plt.show()

df.info()

"""# **Step 19: Splitting the Dataset into Training, Validation, and Testing Sets**

**Data Splitting:**  
To get ready to build and evaluate the machine learning model, we separate the dataset into subsets for training, validation, and testing. This is what happens:
1. **Predictor (`X`) and Target (`y`) Definition:**  
   - `X`: The features used for prediction, such as distances to key locations like schools, hospitals, supermarkets, and more.
   - `y`: The target variable, `Price per Perch`, which we aim to predict.
2. **Splitting into Training and Temporary Sets:**  
   - The dataset is initially split into an 80% training set and a 20% temporary set using `train_test_split`.
3. **Further Splitting of the Temporary Set:**  
   - The temporary set is divided into 80% testing and 20% validation sets to ensure the model is tested and fine-tuned effectively.
4. **Dataset Sizes Printed:**  
   - The sizes of the training, testing, and validation sets are displayed for verification.

This step is essential for ensuring the model is trained, validated, and tested on separate data, improving reliability and generalization.

"""

from sklearn.model_selection import train_test_split

# Define the predictor (X) and target variable (y)
X = df[['Distance from fort', 'min_dist_govtschools_a',
       'min_dist_semigovtschools', 'min_dist_intlschools',
       'min_dist_uni', 'min_dist_nearest_express',
       'min_dist_nearest_railway', 'min_dist_nearest_bank',
        'min_dist_nearest_Govt_Hospital',
        'min_dist_nearest_Pvt_Hospital',
        'min_dist_nearest_Supermarket',
        'min_dist_nearest_Fuel_station']]  # Predictor: Distance from Fort
y = df['Price per Perch']       # Target: Price per Perch

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.8, random_state=42)
# Print the shape of the resulting datasets
print("Training set size (X_train):", X_train.shape)
print("Testing set size (X_test):", X_test.shape)
print("Validating set size (X_valid):", X_valid.shape)

"""# **Step 20: Scaling the Features**

**Feature Scaling:**  
In this step, we scale the features using Min-Max Scaling to ensure all variables are on the same scale, which helps improve model performance. Here's what happens:
1. **Initialize the Scaler:**  
   - The `MinMaxScaler` scales each feature to a range of 0 to 1, preserving the original distribution while eliminating disparities in feature magnitudes.
2. **Fit and Transform Training Data:**  
   - The scaler is fitted to the training data and then used to transform it into the scaled form.
3. **Transform Validation and Test Data:**  
   - The fitted scaler is applied to the validation and test datasets, ensuring consistency with the scaling applied to the training data.

This step is crucial for machine learning algorithms sensitive to feature scales, such as regression and distance-based models.

"""

from sklearn.preprocessing import MinMaxScaler

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Fit the scaler on the training data and transform it
X_train_scaled = scaler.fit_transform(X_train)

# Use the fitted scaler to transform the validation and test data
X_valid_scaled = scaler.transform(X_valid)
X_test_scaled = scaler.transform(X_test)
df.head()

"""# **Step 21: Training and Evaluating the Linear Regression Model**

**Model Training and Evaluation:**  
In this step, we train a Linear Regression model and evaluate its performance using various metrics. Here's what happens:
1. **Model Initialization:**  
   - A `LinearRegression` model is created to predict land prices based on the selected features.
2. **Model Training:**  
   - The model is trained on the training dataset (`X_train` and `y_train`), learning the relationships between the predictors and the target variable.
3. **Predictions:**  
   - The model predicts `Price per Perch` values for the test dataset (`X_test`).
4. **Evaluation Metrics:**  
   - **Mean Absolute Error (MAE):** The average absolute difference between predicted and actual values.
   - **Mean Squared Error (MSE):** The average squared difference between predicted and actual values.
   - **Root Mean Squared Error (RMSE):** The square root of MSE, representing the error in the original units.
   - **R-squared (R²):** Indicates how well the model explains the variance in the target variable (closer to 1 is better).

These metrics help us understand the model's accuracy and identify areas for improvement.

"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Initialize the Linear Regression model
model = LinearRegression()

# Train the model using the training data
model.fit(X_train, y_train)

# Predict values for the test set
y_pred = model.predict(X_test)

# Evaluate the model's performance
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Print the evaluation metrics
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R²): {r2}")

"""# **Step 22: Visualizing Model Predictions**

**Scatter Plot: Actual vs Predicted Values**  
In this stage, a scatter plot is used to show the relationship between the test dataset's actual and predicted values. This is what occurs:
1. **Scatter Plot:**  
   - Each data point represents an actual value (`y_test`) and its corresponding predicted value (`y_pred`) to show how closely the predictions align with the actual values.
2. **Regression Line:**  
   - A red line represents the ideal prediction line, where actual values equal predicted values. Deviations from this line indicate prediction errors.
3. **Labels and Enhancements:**  
   - Titles, axis labels, a legend, and a grid are added to improve clarity and readability.

This visualization helps evaluate the model's performance by showing how well the predictions match the actual values.

"""

import matplotlib.pyplot as plt

# Scatter plot of actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.7, color='blue', label='Data points')

# Plot the regression line
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
         color='red', label='Prediction Line')  # Line from min to max of actual values

# Add titles and labels
plt.title('Regression Line: Actual vs Predicted Values')
plt.xlabel('Actual Values (y_test)')
plt.ylabel('Predicted Values (y_pred)')
plt.legend()
plt.grid(True)

# Show the plot
plt.show()

"""# **Step 23: Analyzing Residuals**

**Residual Plot:**  
In this step, we plot the residuals (difference between actual and predicted values) to evaluate the model's performance further. Here's what happens:
1. **Residual Calculation:**  
   - Residuals are calculated as the difference between actual values (`y_test`) and predicted values (`y_pred`).
2. **Scatter Plot:**  
   - A scatter plot of residuals against predicted values is created to identify patterns or biases in the model's predictions.
3. **Horizontal Line:**  
   - A red dashed line at zero represents the ideal case where residuals are zero (perfect predictions).
4. **Labels and Enhancements:**  
   - Titles, axis labels, and a grid are added for better visualization.

This plot helps check for systematic errors, such as patterns in residuals, which might indicate model issues or the need for further feature engineering.

"""

# Residuals (actual - predicted)
residuals = y_test - y_pred

# Plot residuals
plt.figure(figsize=(10, 6))
plt.scatter(y_pred, residuals, alpha=0.7)
plt.axhline(y=0, color='red', linestyle='--', linewidth=2)
plt.title('Residual Plot')
plt.xlabel('Predicted Price per Perch')
plt.ylabel('Residuals')
plt.grid(True)
plt.show()

"""# **Step 24: Advanced Modeling with Decision Tree and Random Forest**

**Decision Tree and Random Forest Models:**  
In this step, we use advanced models, including Decision Tree and Random Forest, to predict land prices and evaluate their performance. Here's what happens:

### **Decision Tree Regressor**
1. **Model Initialization and Training:**  
   - A `DecisionTreeRegressor` is initialized and trained on the training dataset to predict land prices.
2. **Hyperparameter Tuning with Grid Search:**  
   - A `GridSearchCV` is performed with cross-validation to find the optimal hyperparameters (e.g., `max_depth`, `min_samples_split`, and `min_samples_leaf`) for the Decision Tree model.
3. **Prediction and Evaluation:**  
   - The model predicts land prices for the test dataset, and its performance is evaluated using metrics such as:
     - **Mean Absolute Error (MAE)**
     - **Mean Squared Error (MSE)**
     - **Root Mean Squared Error (RMSE)**
     - **R-squared (R²)**

### **Random Forest Regressor**
1. **Model Initialization and Training:**  
   - A `RandomForestRegressor` with 100 estimators is trained on the training dataset to improve predictive accuracy by combining multiple decision trees.
2. **Log Transformation:**  
   - Log transformation is applied to the target variable (`y_train` and `y_test`) to reduce skewness and improve model performance.
3. **Feature Importance Analysis:**  
   - The feature importance is extracted and visualized using a horizontal bar chart to identify which features contribute most to the predictions.
4. **Prediction and Visualization:**  
   - Predictions are plotted against the actual values in a scatter plot to visualize the model's accuracy and identify any patterns.

This step enhances model performance through hyperparameter tuning and introduces ensemble methods like Random Forest to achieve better predictions and insights.

"""

from sklearn.tree import DecisionTreeRegressor

# Initialize the Decision Tree model
dt_model = DecisionTreeRegressor(random_state=42)

# Train the model using the training data
dt_model.fit(X_train, y_train)

# Predict values for the test set
y_pred_dt = dt_model.predict(X_test)

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeRegressor

param_grid = {
    'max_depth': [5, 10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
}

grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, scoring='neg_mean_squared_error', cv=5)
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_

import numpy as np
y_train_log = np.log1p(y_train)
y_test_log = np.log1p(y_test)
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
feature_importance = model.feature_importances_
plt.barh(X_train.columns, feature_importance)
plt.xlabel("Feature Importance")
plt.show()
plt.scatter(y_test, y_pred, alpha=0.7)
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted')
plt.grid(True)
plt.show()

# Evaluate the model's performance
mae_dt = mean_absolute_error(y_test, y_pred_dt)
mse_dt = mean_squared_error(y_test, y_pred_dt)
rmse_dt = np.sqrt(mse_dt)
r2_dt = r2_score(y_test, y_pred_dt)

# Print the evaluation metrics for Decision Tree
print("Decision Tree Regressor:")
print(f"Mean Absolute Error (MAE): {mae_dt}")
print(f"Mean Squared Error (MSE): {mse_dt}")
print(f"Root Mean Squared Error (RMSE): {rmse_dt}")
print(f"R-squared (R²): {r2_dt}")

"""# **Step 25: Decision Tree Regressor with Evaluation and Visualization**

**Decision Tree Regressor:**  
In this step, we train and evaluate a `DecisionTreeRegressor` model with hyperparameters and visualize its structure. Here's what happens:

### **Training and Evaluation**
1. **Model Training:**  
   - The `DecisionTreeRegressor` is trained on the training dataset (`X_train` and `y_train`).
   - Hyperparameters like `max_depth`, `min_samples_split`, and `min_samples_leaf` are tuned to optimize model performance.
2. **Predictions and Evaluation:**  
   - The model predicts land prices for the test dataset (`y_pred_dt`), and the following metrics are calculated:
     - **Mean Absolute Error (MAE):** Average absolute difference between actual and predicted values.
     - **Mean Squared Error (MSE):** Average squared difference between actual and predicted values.
     - **Root Mean Squared Error (RMSE):** Square root of MSE for easier interpretation.
     - **R-squared (R²):** Proportion of variance in the target variable explained by the model.

### **Feature Importance**
1. **Identifying Key Features:**  
   - The `feature_importances_` attribute is used to extract and print the importance of each predictor, showing which features contribute most to the model's decisions.

### **Cross-Validation**
1. **Model Validation:**  
   - A 5-fold cross-validation is performed using `cross_val_score` to evaluate the model's consistency and calculate the average RMSE.

### **Tree Visualization**
1. **Tree Plot:**  
   - The structure of the decision tree is visualized using `plot_tree`, showing splits, conditions, and outcomes at each node.

This step provides a comprehensive understanding of the Decision Tree model's performance, key contributing features, and internal decision-making process through visualization.

"""

dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)
dt_model.fit(X_train, y_train)  # Ensure this line is run before predicting
y_pred_dt = dt_model.predict(X_test)
# Evaluate the model's performance
mae_dt = mean_absolute_error(y_test, y_pred_dt)
mse_dt = mean_squared_error(y_test, y_pred_dt)
rmse_dt = np.sqrt(mse_dt)
r2_dt = r2_score(y_test, y_pred_dt)

# Print the evaluation metrics for Decision Tree
print("Decision Tree Regressor:")
print(f"Mean Absolute Error (MAE): {mae_dt}")
print(f"Mean Squared Error (MSE): {mse_dt}")
print(f"Root Mean Squared Error (RMSE): {rmse_dt}")
print(f"R-squared (R²): {r2_dt}")
dt_model = DecisionTreeRegressor(
    max_depth=5,               # Maximum depth of the tree
    min_samples_split=10,      # Minimum samples required to split a node
    min_samples_leaf=5,        # Minimum samples required in a leaf node
    random_state=42
)

# Fit the model before accessing feature_importances_
dt_model.fit(X_train, y_train)  # This line is crucial

feature_importance = dt_model.feature_importances_
for i, feature in enumerate(X_train.columns):
    print(f"{feature}: {feature_importance[i]}")
    from sklearn.model_selection import cross_val_score
scores = cross_val_score(dt_model, X_train, y_train, scoring='neg_mean_squared_error', cv=5)
print(f"Average RMSE: {np.sqrt(-scores.mean())}")
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
plt.figure(figsize=(20, 10))
plot_tree(dt_model, feature_names=X_train.columns, filled=True)
plt.show()

"""# **Step 26: Random Forest Regressor for Land Price Prediction**

**Random Forest Regressor:**  
In this step, we train a `RandomForestRegressor` model and evaluate its performance. Here's what happens:

### **Model Training and Prediction**
1. **Model Initialization:**  
   - The `RandomForestRegressor` is initialized with 300 decision trees (`n_estimators=300`) to build an ensemble model for improved accuracy.
2. **Training the Model:**  
   - The model is trained on the training dataset (`X_train` and `y_train`) to learn relationships between features and land prices.
3. **Predictions:**  
   - The trained model predicts `Price per Perch` for the test dataset (`y_pred_rf`).

### **Model Evaluation**
1. **Performance Metrics:**  
   - **Mean Absolute Error (MAE):** Average absolute difference between actual and predicted values.
   - **Mean Squared Error (MSE):** Average squared difference between actual and predicted values.
   - **Root Mean Squared Error (RMSE):** Square root of MSE for interpretation in the same unit as the target variable.
   - **R-squared (R²):** Proportion of variance in the target variable explained by the model (closer to 1 is better).

This step uses the power of ensemble learning to enhance the predictive accuracy and reliability of the model for land price estimation.

"""

from sklearn.ensemble import RandomForestRegressor

# Initialize the Random Forest model
rf_model = RandomForestRegressor(random_state=42, n_estimators=300)

# Train the model using the training data
rf_model.fit(X_train, y_train)

# Predict values for the test set
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model's performance
mae_rf = mean_absolute_error(y_test, y_pred_rf)
mse_rf = mean_squared_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mse_rf)
r2_rf = r2_score(y_test, y_pred_rf)

# Print the evaluation metrics for Random Forest
print("Random Forest Regressor:")
print(f"Mean Absolute Error (MAE): {mae_rf}")
print(f"Mean Squared Error (MSE): {mse_rf}")
print(f"Root Mean Squared Error (RMSE): {rmse_rf}")
print(f"R-squared (R²): {r2_rf}")

"""# **Step 27: Extracting and Sorting Feature Importance**

**Feature Importance:**  
In this step, we analyze the importance of each feature in the `RandomForestRegressor` model. Here's what happens:
1. **Extracting Feature Importance:**  
   - The `feature_importances_` attribute of the trained `RandomForestRegressor` model is used to determine how much each feature contributes to the predictions.
2. **Creating a DataFrame:**  
   - A DataFrame is created with two columns:
     - **`Features`**: The names of the predictor variables.
     - **`Importance`**: The corresponding importance scores assigned by the model.
3. **Sorting Features:**  
   - The features are sorted in descending order of importance, highlighting the most influential predictors for land price estimation.

This step provides insights into which features have the greatest impact on the target variable, helping refine the model or focus on key predictors in the dataset.

"""

feat_imp = pd.DataFrame()
feat_imp['Features'] = X_train.columns
feat_imp['Importance'] = rf_model.feature_importances_
feat_imp = feat_imp.sort_values(by='Importance', ascending=False)

feat_imp

"""# **Step 28: CatBoost Regressor for Land Price Prediction**

**CatBoost Regressor:**  
In this step, we use the `CatBoostRegressor` model, a gradient boosting algorithm optimized for handling categorical data, to predict land prices. Here's what happens:

### **Model Training and Prediction**
1. **Model Initialization:**  
   - The `CatBoostRegressor` is initialized with default parameters for robust and efficient training.
2. **Training the Model:**  
   - The model is trained on the training dataset (`X_train` and `y_train`) to learn relationships between the features and land prices.
3. **Predictions:**  
   - The trained model predicts `Price per Perch` for the test dataset (`y_pred_rf`).

### **Model Evaluation**
1. **Performance Metrics:**  
   - **Mean Absolute Error (MAE):** Measures the average absolute difference between predicted and actual values.
   - **Mean Squared Error (MSE):** Measures the average squared difference between predicted and actual values.
   - **Root Mean Squared Error (RMSE):** Represents the prediction error in the same unit as the target variable.
   - **R-squared (R²):** Indicates the proportion of variance in the target variable explained by the model.

This step leverages the `CatBoostRegressor` to potentially improve predictive accuracy, especially when dealing with complex relationships in the data.

"""

!pip install catboost # Install the catboost library

from sklearn.ensemble import RandomForestRegressor
from catboost import CatBoostRegressor # Now you can import CatBoostRegressor

# Initialize the CatBoost model
rf_model = CatBoostRegressor(random_state=0, verbose = 0)

# Train the model using the training data
rf_model.fit(X_train, y_train)

# Predict values for the test set
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model's performance
mae_rf = mean_absolute_error(y_test, y_pred_rf)
mse_rf = mean_squared_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mse_rf)
r2_rf = r2_score(y_test, y_pred_rf)

# Print the evaluation metrics for Random Forest
print("Random Forest Regressor:")
print(f"Mean Absolute Error (MAE): {mae_rf}")
print(f"Mean Squared Error (MSE): {mse_rf}")
print(f"Root Mean Squared Error (RMSE): {rmse_rf}")
print(f"R-squared (R²): {r2_rf}")

"""# **Step 29: Support Vector Regression (SVR) for Land Price Prediction**

**Support Vector Regression (SVR):**  
In this step, we use a `Support Vector Regression` (SVR) model to predict land prices. SVR is effective for capturing non-linear relationships in the data. Here's what happens:

### **Model Training and Prediction**
1. **Model Initialization:**  
   - The SVR model is initialized with:
     - **Kernel:** `'rbf'` (Radial Basis Function) for non-linear relationships.
     - **C:** Regularization parameter set to 100 to control overfitting.
     - **Epsilon:** 0.1, defining a margin of tolerance where predictions are considered correct without penalty.
2. **Training the Model:**  
   - The model is trained on the training dataset (`X_train` and `y_train`).
3. **Predictions:**  
   - The trained model predicts `Price per Perch` for the test dataset (`y_pred_svr`).

### **Model Evaluation**
1. **Performance Metrics:**  
   - **Mean Absolute Error (MAE):** Average absolute difference between actual and predicted values.
   - **Mean Squared Error (MSE):** Average squared difference between actual and predicted values.
   - **Root Mean Squared Error (RMSE):** Square root of MSE for easier interpretation.
   - **R-squared (R²):** Proportion of variance in the target variable explained by the model (closer to 1 is better).

This step uses SVR to explore its effectiveness in capturing complex relationships and improving prediction accuracy.

"""

from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Initialize the SVR model
# You can customize the kernel ('linear', 'poly', 'rbf', or 'sigmoid') and hyperparameters (e.g., C, epsilon).
# Ensure your dataset is split into training and testing sets
# Example placeholder (replace X and y with your actual data variables)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 1: Feature Scaling (Important for SVR)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 2: GridSearchCV for Hyperparameter Tuning
param_grid = {
    'C': [1, 10, 100, 1000],
    'epsilon': [0.1, 0.2, 0.5, 1.0],
    'kernel': ['linear', 'rbf']
}

svr_model = SVR()
grid_search = GridSearchCV(svr_model, param_grid, cv=5, scoring='r2')
grid_search.fit(X_train, y_train)

# Best hyperparameters
best_params = grid_search.best_params_
print("Best Parameters from GridSearchCV:", best_params)

# Step 3: Train the SVR Model with Best Parameters
svr_optimized = SVR(kernel=best_params['kernel'], C=best_params['C'], epsilon=best_params['epsilon'])
svr_optimized.fit(X_train, y_train)

# Step 4: Predict Values for the Test Set
y_pred_svr = svr_optimized.predict(X_test)

# Step 5: Evaluate Model Performance
mae_svr = mean_absolute_error(y_test, y_pred_svr)
mse_svr = mean_squared_error(y_test, y_pred_svr)
rmse_svr = np.sqrt(mse_svr)
r2_svr = r2_score(y_test, y_pred_svr)

print("Support Vector Regression (Optimized):")
print(f"Mean Absolute Error (MAE): {mae_svr}")
print(f"Mean Squared Error (MSE): {mse_svr}")
print(f"Root Mean Squared Error (RMSE): {rmse_svr}")
print(f"R-squared (R²): {r2_svr}")

# Predicted vs Actual Plot
plt.scatter(y_test, y_pred_svr, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Predicted vs Actual Values (SVR)')
plt.show()

"""# **Step 30: K-Nearest Neighbors (KNN) Regression for Land Price Prediction**

**KNN Regression:**  
In this step, we use the `KNeighborsRegressor` model to predict land prices. KNN regression predicts values based on the average of the nearest data points. Here's what happens:

### **Model Training and Prediction**
1. **Model Initialization:**  
   - The KNN model is initialized with:
     - **n_neighbors:** 5, meaning predictions are based on the 5 nearest neighbors.
     - **weights:** `'distance'`, giving more weight to closer neighbors.
2. **Training the Model:**  
   - The model is trained on the training dataset (`X_train` and `y_train`).
3. **Predictions:**  
   - The trained model predicts `Price per Perch` for the test dataset (`y_pred_knn`).

### **Model Evaluation**
1. **Performance Metrics:**  
   - **Mean Absolute Error (MAE):** Average absolute difference between actual and predicted values.
   - **Mean Squared Error (MSE):** Average squared difference between actual and predicted values.
   - **Root Mean Squared Error (RMSE):** Square root of MSE for easier interpretation.
   - **R-squared (R²):** Proportion of variance in the target variable explained by the model (closer to 1 is better).

This step uses KNN regression to evaluate its effectiveness in capturing relationships in the data and predicting land prices accurately.

"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Initialize the KNN Regressor
# You can customize the number of neighbors (n_neighbors) and weights ('uniform' or 'distance').
knn_model = KNeighborsRegressor(n_neighbors=5, weights='distance')

# Ensure X_train and y_train are loaded and ready
# Check shapes before fitting
print(X_train.shape)
print(y_train.shape)

# Train the KNN model using the training data
knn_model.fit(X_train, y_train)

# Predict values for the test set
y_pred_knn = knn_model.predict(X_test)

# Evaluate the model's performance
mae_knn = mean_absolute_error(y_test, y_pred_knn)
mse_knn = mean_squared_error(y_test, y_pred_knn)
rmse_knn = np.sqrt(mse_knn)
r2_knn = r2_score(y_test, y_pred_knn)

# Print the evaluation metrics for KNN
print("K-Nearest Neighbors Regression (KNN):")
print(f"Mean Absolute Error (MAE): {mae_knn}")
print(f"Mean Squared Error (MSE): {mse_knn}")
print(f"Root Mean Squared Error (RMSE): {rmse_knn}")
print(f"R-squared (R²): {r2_knn}")

"""# **Step 31: Comparing Model Performance**

**Model Performance Comparison:**  
In this step, we compare the performance of all trained models using various metrics to identify the best-performing one. Here's what happens:

### **Metrics Visualization**
1. **Mean Absolute Error (MAE):**  
   - A bar chart compares the average absolute prediction errors for each model. Lower values indicate better performance.
2. **Mean Squared Error (MSE):**  
   - A bar chart compares the average squared prediction errors. Lower values indicate better accuracy.
3. **Root Mean Squared Error (RMSE):**  
   - A bar chart shows the square root of the MSE for each model, making it easier to interpret in the same unit as the target variable.
4. **R-squared (R²):**  
   - A bar chart compares the proportion of variance explained by each model. Higher values (closer to 1) indicate better predictive power.

### **Purpose of Comparison**
This step provides a clear visual representation of each model's strengths and weaknesses, helping us choose the most suitable model for land price prediction.

"""

# Compare the models' performance visually
models = ['Linear Regression', 'Decision Tree', 'Random Forest', 'Support Vector Regression', 'KNN']
mae_scores = [mae, mae_dt, mae_rf, mae_svr, mae_knn]
mse_scores = [mse, mse_dt, mse_rf, mse_svr, mse_knn]
rmse_scores = [rmse, rmse_dt, rmse_rf, rmse_svr, rmse_knn]
r2_scores = [r2, r2_dt, r2_rf, r2_svr, r2_knn]

# MAE Comparison
plt.figure(figsize=(10, 6))
plt.bar(models, mae_scores, color='blue', alpha=0.7)
plt.title('Mean Absolute Error (MAE) Comparison')
plt.ylabel('MAE')
plt.grid(axis='y')
plt.show()

# MSE Comparison
plt.figure(figsize=(10, 6))
plt.bar(models, mse_scores, color='red', alpha=0.7)
plt.title('Mean Squared Error (MAE) Comparison')
plt.ylabel('MSE')
plt.grid(axis='y')
plt.show()

# RMSE Comparison
plt.figure(figsize=(10, 6))
plt.bar(models, rmse_scores, color='green', alpha=0.7)
plt.title('Root Mean Squared Error (RMSE) Comparison')
plt.ylabel('RMSE')
plt.grid(axis='y')
plt.show()

# R² Comparison
plt.figure(figsize=(10, 6))
plt.bar(models, r2_scores, color='orange', alpha=0.7)
plt.title('R-squared (R²) Comparison')
plt.ylabel('R²')
plt.grid(axis='y')
plt.show()

"""# **Step 32: Predicting Prices for New Data**

**Price Prediction for New Distances:**  
In this step, we use the trained `RandomForestRegressor` model to predict land prices for a range of distances. Here's what happens:

### **Generating New Data**
1. **Create a DataFrame for Distance Ranges:**  
   - A range of values from 1 km to 30 km is generated for multiple distance-related variables, such as:
     - `Distance from fort`
     - `min_dist_govtschools_a`
     - `min_dist_semigovtschools`
     - Other relevant features.

### **Prediction**
2. **Predict Prices:**  
   - The trained `RandomForestRegressor` model is used to predict the `Price per Perch` for the generated distances.

### **Formatting and Display**
3. **Format Predictions:**  
   - The predicted prices are formatted as currency (e.g., `100.00 LKR`) for readability.
4. **Create a Table:**  
   - A DataFrame is created to display the distances alongside their corresponding predicted prices.

### **Purpose**
This step demonstrates the model's ability to generalize by predicting prices for new scenarios, providing insights into how land prices might vary with changing proximities to key locations.

"""

import pandas as pd

# Generate a range of distances from 1 km to 30 km for multiple variables
new_distances = pd.DataFrame({
    'Distance from fort': range(1, 31),
    'min_dist_govtschools_a': range(1, 31),
    'min_dist_semigovtschools': range(1, 31),
    'min_dist_intlschools': range(1, 31),
    'min_dist_uni': range(1, 31),
    'min_dist_nearest_express': range(1, 31),
    'min_dist_nearest_railway': range(1, 31),
    'min_dist_nearest_bank': range(1, 31),
    'min_dist_nearest_Govt_Hospital': range(1, 31),
    'min_dist_nearest_Pvt_Hospital': range(1, 31),
    'min_dist_nearest_Supermarket': range(1, 31),
    'min_dist_nearest_Fuel_station': range(1, 31)
})

# Predict prices using the trained Random Forest model
predicted_prices = rf_model.predict(new_distances)

# Prepare the table with predictions and format prices as "100.00 LKR"
predictions = pd.DataFrame({
    'Distance from Fort (km)': new_distances['Distance from fort'],
    'min_dist_govtschools_a': new_distances['min_dist_govtschools_a'],
    'min_dist_semigovtschools': new_distances['min_dist_semigovtschools'],
    'min_dist_intlschools': new_distances['min_dist_intlschools'],
    'min_dist_uni': new_distances['min_dist_uni'],
    'min_dist_nearest_express': new_distances['min_dist_nearest_express'],
    'min_dist_nearest_railway': new_distances['min_dist_nearest_railway'],
    'min_dist_nearest_bank': new_distances['min_dist_nearest_bank'],
    'min_dist_nearest_Govt_Hospital': new_distances['min_dist_nearest_Govt_Hospital'],
    'min_dist_nearest_Pvt_Hospital': new_distances['min_dist_nearest_Pvt_Hospital'],
    'min_dist_nearest_Supermarket': new_distances['min_dist_nearest_Supermarket'],
    'min_dist_nearest_Fuel_station': new_distances['min_dist_nearest_Fuel_station'],
    'Predicted Price per Perch (LKR)': [f"{price:,.2f} LKR" for price in predicted_prices]
})

# Display the table
print(predictions)

"""# **Step 33: Visualizing Predicted Prices Against Features**

**Line and Scatter Plots for Predictions:**  
In this step, we visualize how the predicted land prices vary with different distance-related features using combined line and scatter plots. Here's what happens:

### **Visualization**
1. **Select Features for Analysis:**  
   - A list of features (excluding `Distance from Fort`) is created to examine their impact on predicted prices, including:
     - `min_dist_govtschools_a`
     - `min_dist_semigovtschools`
     - `min_dist_intlschools`
     - Other proximity metrics.
2. **Loop Through Features:**  
   - For each feature, a plot is generated to show:
     - **Line Plot:** Represents the predicted prices across the range of feature values.
     - **Scatter Plot:** Shows individual predicted price points for clarity.
3. **Customizations:**  
   - Titles, axis labels, legends, and gridlines are added for better readability.
   - X-axis ticks are customized to improve visualization.

### **Purpose**
This step helps:
1. Identify how changes in proximity to various locations impact predicted land prices.
2. Gain insights into the relationships between features and prices for better decision-making.

"""

# List of feature names (excluding 'Distance from Fort')
features = [
    'min_dist_govtschools_a', 'min_dist_semigovtschools',
    'min_dist_intlschools', 'min_dist_uni', 'min_dist_nearest_express',
    'min_dist_nearest_railway', 'min_dist_nearest_bank',
    'min_dist_nearest_Govt_Hospital', 'min_dist_nearest_Pvt_Hospital', 'min_dist_nearest_Supermarket', 'min_dist_nearest_Fuel_station'
]

# Loop over each feature to create the combined line and scatter plot
for feature in features:
    plt.figure(figsize=(12, 6))
    plt.plot(new_distances[feature], predicted_prices, color='green', label=f'Predicted Prices vs {feature}')
    plt.scatter(new_distances[feature], predicted_prices, color='red', alpha=0.8, label=f'{feature} Price Points')
    plt.title(f'Predicted Prices vs {feature}', fontsize=16)
    plt.xlabel(f'{feature} (km)', fontsize=14)
    plt.ylabel('Predicted Price per Perch (Rs)', fontsize=14)
    plt.legend(fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.xticks(range(1, 31, 2))  # Adjust x-ticks as per your preference
    plt.show()

"""# **Step 34: Building an Interactive Land Price Prediction Tool**

**Interactive Prediction Tool:**  
In this step, we create an interactive tool using the Gradio library to predict land prices based on user-provided inputs for various proximity metrics. Here's what happens:

### **Preparation**
1. **Simulated Data and Model Training:**  
   - A simulated dataset is created with features like distances to schools, hospitals, supermarkets, and other key locations.
   - A `RandomForestRegressor` is trained on this dataset to predict `Price per Perch`.

### **Prediction Function**
2. **Defining the Prediction Function:**  
   - A Python function `predict_price` is created to take user inputs, format them, and predict the price using the trained model.
   - The function returns the predicted price formatted as currency for user readability.

### **Gradio Interface**
3. **Setting Up Gradio:**  
   - Gradio's `Interface` is used to create an interactive web interface for predictions.
   - Users can enter distances for various features through input fields, such as:
     - `Distance from Fort`
     - `min_dist_govtschools_a`
     - `min_dist_nearest_Supermarket`, etc.
   - The output is displayed as the predicted price per perch.

4. **Launching the Interface:**  
   - The Gradio interface is launched, allowing users to interact with the model and receive predictions in real-time.

### **Purpose**
This step demonstrates the practical application of the trained model, enabling users to estimate land prices dynamically based on their input values. It bridges the gap between machine learning models and user-friendly interfaces.

"""

# Install required libraries
!pip install gradio xgboost

# Import required libraries
import gradio as gr
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
import pandas as pd
import matplotlib.pyplot as plt

# Simulate training the Random Forest model with multiple features
data = {
    'Distance from fort': range(1, 31),
    'min_dist_govtschools_a': [1 + x * 0.2 for x in range(1, 31)],
    'min_dist_semigovtschools': [3 + x * 0.1 for x in range(1, 31)],
    'min_dist_intlschools': [4 + x * 0.5 for x in range(1, 31)],
    'min_dist_uni': [5 + x * 0.2 for x in range(1, 31)],
    'min_dist_nearest_express': [1.5 + x * 0.4 for x in range(1, 31)],
    'min_dist_nearest_railway': [2.5 + x * 0.2 for x in range(1, 31)],
    'min_dist_nearest_bank': [3.5 + x * 0.3 for x in range(1, 31)],
    'min_dist_nearest_Govt_Hospital': [1.8 + x * 0.1 for x in range(1, 31)],
    'min_dist_nearest_Pvt_Hospital': [2.8 + x * 0.5 for x in range(1, 31)],
    'min_dist_nearest_Supermarket': [4.5 + x * 0.6 for x in range(1, 31)],
    'min_dist_nearest_Fuel_station': [5.5 + x * 0.3 for x in range(1, 31)],
    'Price per Perch': [6_000_000 - 200_000 * x for x in range(1, 31)]
}

df = pd.DataFrame(data)

# Features and target variable
X = df.drop(columns=['Price per Perch'])
y = df['Price per Perch']

# Train the Random Forest model
rf_model = RandomForestRegressor(random_state=42, n_estimators=100)
rf_model.fit(X, y)

# Analyze Feature Importance
feature_importances = rf_model.feature_importances_
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

print("Feature Importances:")
print(importance_df)

# Visualize how Distance from fort affects Price per Perch
import numpy as np
predicted_prices = []
distances = np.linspace(1, 30, 30)
for distance in distances:
    # Use mean values for all other features
    input_features = [
        distance,
        X['min_dist_govtschools_a'].mean(),
        X['min_dist_semigovtschools'].mean(),
        X['min_dist_intlschools'].mean(),
        X['min_dist_uni'].mean(),
        X['min_dist_nearest_express'].mean(),
        X['min_dist_nearest_railway'].mean(),
        X['min_dist_nearest_bank'].mean(),
        X['min_dist_nearest_Govt_Hospital'].mean(),
        X['min_dist_nearest_Pvt_Hospital'].mean(),
        X['min_dist_nearest_Supermarket'].mean(),
        X['min_dist_nearest_Fuel_station'].mean()
    ]
    predicted_prices.append(rf_model.predict([input_features])[0])

plt.figure(figsize=(10, 6))
plt.plot(distances, predicted_prices, marker='o')
plt.title('Predicted Price per Perch vs Distance from Fort')
plt.xlabel('Distance from Fort (km)')
plt.ylabel('Predicted Price per Perch (Rs)')
plt.grid()
plt.show()

# Define the prediction function
def predict_price(*inputs):
    # Prepare the input data in the same format as the model expects
    input_data = [list(inputs)]
    price = rf_model.predict(input_data)[0]
    return f"The predicted price per perch is: Rs {price:,.2f}"

# Create the Gradio interface
interface = gr.Interface(
    fn=predict_price,
    inputs=[
        gr.Number(label="Enter Distance from Fort (km)", value=1),
        gr.Number(label="Enter min_dist_govtschools_a (km)", value=1),
        gr.Number(label="Enter min_dist_semigovtschools (km)", value=1),
        gr.Number(label="Enter min_dist_intlschools (km)", value=1),
        gr.Number(label="Enter min_dist_uni (km)", value=1),
        gr.Number(label="Enter min_dist_nearest_express (km)", value=1),
        gr.Number(label="Enter min_dist_nearest_railway (km)", value=1),
        gr.Number(label="Enter min_dist_nearest_bank (km)", value=1),
        gr.Number(label="Enter min_dist_nearest_Govt_Hospital (km)", value=1),
        gr.Number(label="Enter min_dist_nearest_Pvt_Hospital (km)", value=1),
        gr.Number(label="Enter min_dist_nearest_Supermarket (km)", value=1),
        gr.Number(label="Enter min_dist_nearest_Fuel_station (km)", value=1)
    ],
    outputs=gr.Textbox(label="Predicted Price per Perch")
)

# Launch the interface
interface.launch()